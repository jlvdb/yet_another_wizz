#!/usr/bin/env python3
import argparse
import os
import shutil
import sys
from collections import Counter
from functools import reduce

import numpy as np
from yaw_tools.folders import CCFolder
from yaw_tools.region_counts import merge_region_counts
from yet_another_wizz.utils import dump_json, load_json


parser = argparse.ArgumentParser(
    description='Merges region count files produced by yet_another-wizz')
parser.add_argument(
    '-i', '--input', nargs='*', required=True,
    help='input folders which contain output of yet_another')
parser.add_argument(
    '--scale', help='only merge this specific scale (ignores missing input)')
parser.add_argument(
    '-o', '--output', required=True,
    help='path to folder where merged output is stored')


if __name__ == "__main__":
    args = parser.parse_args()

    # check if all the input exists
    folders = []
    for folder in args.input:
        abspath = os.path.abspath(os.path.expanduser(folder))
        if not os.path.exists(abspath):
            raise OSError("input folder not found: %s" % abspath)
        folders.append(CCFolder(abspath))
    print("==> processing %d input folders" % len(folders))
    print("output folder: %s" % args.output)

    print("checking binning")
    binning = None
    for folder in folders:
        try:
            next_binning = np.loadtxt(folder.path_binning_file())
        except OSError:
            raise ValueError(
                "folder does not contain yet_another_wizz output: %s" %
                folder.root)
        if binning is None:
            binning = next_binning
        elif not np.all(binning == next_binning):
            raise ValueError(
                "binning does not match: %s" % folder.path_binning_file())

    # create the output folder and copy over one of the binning files
    outdir = CCFolder(args.output)
    if not os.path.exists(outdir.path_binning_file()):
        shutil.copy(
            folder.path_binning_file(),  # all the same in each folder
            outdir.path_binning_file())
    # check if any existing binning matches the input data
    if os.path.exists(outdir.path_binning_file()):
        old_binning = np.loadtxt(folder.path_binning_file())
        new_binning = np.loadtxt(outdir.path_binning_file())
        if not np.all(old_binning == new_binning):
            raise ValueError(
                "data in output folder exists but has different redshift " +
                "binning: " + outdir.path_binning_file())

    # check which scales coexist
    print("finding correlation scales")
    scale_counter = Counter()
    for folder in folders:
        scale_counter.update(folder.list_scalenames())
    all_scales = set(scale_counter.keys())
    common_scales = set(
        scale for scale, count in scale_counter.items()
        if count == len(folders))
    print("found scales: %s" % str(all_scales))
    if args.scale is None:
        print("selecting:    %s" % str(common_scales))
        scales = list(common_scales)
    else:  # use only requested scale
        print("selecting:    %s" % str(args.scale))
        if args.scale not in all_scales:
            raise ValueError("scale '%s' not found" % args.scale)
        if args.scale not in common_scales:
            print(
                "WARNING: scale '%s' is not common to all input" % args.scale)
        scales = [args.scale]
    if len(scales) == 0:
        sys.exit("WARNING: there is nothing to do")
    # create the required scale folders at the output destination
    for scale in scales:
        outdir.add_scale(scale)

    # remove scale folders that were not selected
    scale_folders = {}
    for scale in scales:
        scale_folders[scale] = []
        for folder in folders:
            if scale in folder:
                scale_folders[scale].append(folder[scale])

    for scale in scales:
        print("==> processing scale: %s" % scale)
        N_folders = len(scale_folders[scale])

        # check which correlation region counts exist
        zbin_counter = Counter()
        suffix_counter = Counter()
        for folder in scale_folders[scale]:
            zbin_dict = folder.list_crosscorr_files(".json")
            zbin_counter.update(zbin_dict.keys())
            suffix_dict = folder.list_autocorr_files(".json")
            suffix_counter.update(suffix_dict.keys())
        zbins = set(zbin_counter.keys())
        suffixes = set(suffix_counter.keys())
        # check that each folder contains all cross-correlation redshift bin
        for zbin, count in zbin_counter.items():
            if count != N_folders:
                string = "missing cross-correlation region counts for "
                string += "bin '%s' " % zbin
                string += "in %d / %d input folders" % (
                    N_folders - count, N_folders)
                raise ValueError(string)
        # check that each folder contains all auto-correlation suffixes
        for suffix, count in suffix_counter.items():
            if count != N_folders:
                string = "missing auto-correlation region counts for "
                string += "suffix '%s' " % suffix
                string += "in %d / %d input folders" % (
                    N_folders - count, N_folders)
                raise ValueError(string)

        # merge the region counts
        if len(zbins) == 0 and len(suffixes) == 0:
            sys.exit("WARNING: there is nothing to do")
 
        if len(suffixes) > 0:
            print("merging auto-correlation region counts")
            print("found auto-correlation types: %s" % str(sorted(suffixes)))
            for suffix in suffixes:
                paths = [
                    folder.path_autocorr_file(".json", suffix=suffix)
                    for folder in scale_folders[scale]]
                master_counts_dict = merge_region_counts(paths)
                outpath = outdir[scale].path_autocorr_file(
                    ".json", suffix=suffix)
                print("writing merged counts: %s" % os.path.basename(outpath))
                dump_json(master_counts_dict, outpath)

        if len(zbins) > 0:
            print("merging cross-correlation region counts")
            print("found tomographic bins: %s" % str(sorted(zbins)))
            for zbin in zbins:
                paths = [
                    folder.path_crosscorr_file(".json", zlims=zbin)
                    for folder in scale_folders[scale]]
                master_counts_dict = merge_region_counts(paths)
                outpath = outdir[scale].path_crosscorr_file(
                    ".json", zlims=zbin)
                print("writing merged counts: %s" % os.path.basename(outpath))
                dump_json(master_counts_dict, outpath)

        # sum the bin weights together
        weights_exist = [
            os.path.exists(folder.path_weights_file())
            for folder in scale_folders[scale]]
        if not any(weights_exist):
            print("found no tomographic bin weights to sum")
        elif not all(weights_exist):
            raise ValueError("not all input folders have bin weight files")
        else:
            print("compute the combined tomographic bin weights")
            weights = []
            for folder in scale_folders[scale]:
                data = load_json(folder.path_weights_file())
                if set(data.keys()) != zbins:
                    raise ValueError(
                        "not all bins have an associated weight")
                # Counters are like dicts but can be summed
                weights.append(Counter(data))
            weight_sum = dict(reduce(lambda x, y : x + y, weights))
            print(
                "weight sum:   %s" % {
                    k: "%.2e" % v for k, v in weight_sum.items()})
            dump_json(weight_sum, outdir[scale].path_weights_file())
