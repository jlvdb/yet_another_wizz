#!/usr/bin/env python3
import argparse
import os
import warnings
from collections import Counter
from hashlib import md5

import numpy as np
from numpy import ma
from yaw_tools.folders import binname, CCFolder
from yaw_tools.pickles import AutoCorrelationPickle, CrossCorrelationPickle


def nancov(bootstraps):
    # mask infinite values
    mask = np.logical_not(np.isfinite(bootstraps))
    masked_boots = ma.array(bootstraps, mask=mask)
    # compute covariance
    covar = ma.cov(masked_boots, ddof=0)
    # find rows/columns with NaNs
    diag = np.diag(covar)
    idx = np.arange(len(diag))
    idx = idx[np.isnan(diag)]
    # set the diag. element to infinity and the off-diag elements to 0
    for i in idx:
        covar[i, :] = 0.0
        covar[:, i] = 0.0
        covar[i, i] = np.inf
    return covar


parser = argparse.ArgumentParser(
    description='Unpickles yet_another_wizz region pickle files, optionally '
                'applying bias corrections.')
parser.add_argument(
    'wdir', metavar='DATAFOLDER',
    help='an output folder of yet_another_wizz')
parser.add_argument(
    '--bias-spec',
    help='suffix of the auto-correlation files to apply for correcting the '
         'spectroscopic bias (optional)')
parser.add_argument(
    '--bias-phot',
    help='suffix of the auto-correlation files to apply for correcting the '
         'photometric bias (optional)')
parser.add_argument(
    '--seed',
    help='string from which a seed for the random generator is computed')
parser.add_argument(
    '--extension', default="yaw",
    help='file extension name of the output files (default: %(default)s)')
parser.add_argument(
    '--cov-order', nargs='*',
    help='order of redshift keys (e.g. 0.101z1.201) in which the cross-'
         'correlation files are inserted into the global covariance matrix, '
         'if not given no global covariance is computed (default: try to '
         'locate ".order" file)')
parser.add_argument(
    '-o', '--output',
    help='folder in which the output is stored (optional)')


if __name__ == "__main__":

    args = parser.parse_args()

    setattr(args, "wdir", os.path.abspath(os.path.expanduser(args.wdir)))
    if not os.path.exists(args.wdir):
        raise OSError("input folder does not exist")
    print("==> processing data in: %s" % args.wdir)
    indir = CCFolder(args.wdir)

    # check if a binning file and which scale dirs exits
    if not os.path.exists(indir.path_binning_file()):
        raise ValueError("input folder does not contain valid output")

    print("finding correlation scales")
    scales = set(indir.list_scalenames())
    if len(scales) == 0:
        raise ValueError("input folder contains no scales")
    print("found scales: %s" % set(os.path.basename(s) for s in scales))

    # convert the pickles
    for scale, scaledir in indir.iter_scales():
        print("==> processing scale: %s" % scale)

        # check which correlation pickles exist
        cc_files = scaledir.list_crosscorr_files(".pickle").values()
        if len(cc_files) == 0:
            raise ValueError("no cross-correlation pickles found")
        ac_dict = scaledir.list_autocorr_files(".pickle")
        if args.bias_spec is not None:
            try:
                spec_ac_pickle = ac_dict[args.bias_spec]
                print("found spec. auto-correlation pickle")
            except KeyError:
                raise ValueError("spec. auto-correlation pickle not found")
        else:
            spec_ac_pickle = None
        if args.bias_phot is not None:
            try:
                phot_ac_pickle = ac_dict[args.bias_phot]
                print("found phot. auto-correlation pickle")
            except KeyError:
                raise ValueError("phot. auto-correlation pickle not found")
        else:
            phot_ac_pickle = None

        print(scaledir.root)

        # now everything should be good and we can produce output
        if args.output is None:
            outdir = indir
        else:
            outdir = indir.copy_meta_to(args.output)

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            # load the auto-correlation pickles
            idx = None
            if spec_ac_pickle is not None:
                ac_spec = AutoCorrelationPickle(spec_ac_pickle)
            if phot_ac_pickle is not None:
                ac_phot = AutoCorrelationPickle(phot_ac_pickle)
            y_samples_global = {}
            # process the cross-correlation pickles
            hasher = md5(bytes(args.seed, "utf-8"))
            hashval = bytes(hasher.hexdigest(), "utf-8")
            np.random.seed(np.frombuffer(hashval, dtype=np.uint32))
            print(cc_files)
            for path in cc_files:
                cc = CrossCorrelationPickle(path)
                key = "w_sp"
                if idx is None:  # generate bootstrap samples
                    cc.generate_sampling_idx(n_bootstraps=1000)
                    idx = cc.get_sampling_idx()
                    if args.bias_spec:
                        ac_spec.set_sampling_idx(idx)
                    if args.bias_phot:
                        ac_phot.set_sampling_idx(idx)
                cc.set_sampling_idx(idx)  # apply the same sampling everywhere
                # compute the cross-correlation redshifts
                z = cc.get_redshifts()
                y = cc.get_amplitudes()
                y_samples = cc.get_samples()
                bias = np.ones_like(y)
                bias_samples = np.ones_like(y_samples)
                if args.bias_spec:
                    key += " / bias_spec"
                    bias *= ac_spec.get_amplitudes()
                    bias_samples *= ac_spec.get_samples()
                if args.bias_phot:
                    key += " / bias_phot"
                    bias *= ac_phot.get_amplitudes()
                    bias_samples *= ac_phot.get_samples()
                y /= np.sqrt(bias)
                y_samples /= np.sqrt(bias_samples)
                y_samples_global[binname(path)] = y_samples.copy()
                err = np.nanstd(y_samples, axis=1)
                nz_array = np.stack([z, y, err]).T
                cov = nancov(y_samples)
                # write data to file
                outpath = outdir[scale].incorporate(path, args.extension)
                print(
                    "writing redshift distribution to: %s" %
                    os.path.basename(outpath))
                header = "col 1 = mean redshift\n"
                header += "col 2 = correlation amplitude (%s)\n" % key
                header += "col 3 = amplitude error"
                np.savetxt(outpath, nz_array, header=header)
                # write covariance matrix
                outpath = outdir[scale].incorporate(path, ".cov")
                print(
                    "writing covariance matrix to:     %s" %
                    os.path.basename(outpath))
                header = "correlation amplitude (%s) covariance matrix" % key
                np.savetxt(outpath, cov, header=header)
            # process auto-correlation pickles
            if spec_ac_pickle is not None:
                wz_array = np.stack([
                    ac_spec.get_redshifts(),
                    ac_spec.get_amplitudes(),
                    ac_spec.get_errors()]).T
                outpath = outdir[scale].incorporate(
                    spec_ac_pickle, args.extension)
                print(
                    "writing auto-correlation to:      %s" %
                    os.path.basename(outpath))
                header = "col 1 = mean redshift\n"
                header += "col 2 = correlation amplitude (w_ss)\n"
                header += "col 3 = amplitude error"
                np.savetxt(outpath, wz_array, header=header)
            if phot_ac_pickle is not None:
                wz_array = np.stack([
                    ac_phot.get_redshifts(),
                    ac_phot.get_amplitudes(),
                    ac_phot.get_errors()]).T
                outpath = outdir[scale].incorporate(
                    phot_ac_pickle, args.extension)
                print(
                    "writing auto-correlation to:      %s" %
                    os.path.basename(outpath))
                header = "col 1 = mean redshift\n"
                header += "col 2 = correlation amplitude (w_pp)\n"
                header += "col 3 = amplitude error"
                np.savetxt(outpath, wz_array, header=header)
            # compute global covariance matrix
            if args.cov_order is None:
                print("omitting global covariance estimation")
            else:
                data_keys = y_samples_global.keys()
                print(args.cov_order, data_keys)
                y_global = None
                for key in args.cov_order:
                    dkey = "#####"
                    for data_key in data_keys:
                        if key in data_key:
                            dkey = data_key
                            break
                    if dkey not in data_keys:
                        raise ValueError(
                            "redshift key '%s' not in data" % dkey)
                    y_samples = y_samples_global[dkey]
                    if y_global is None:
                        y_global = y_samples.copy()
                    else:
                        y_global = np.concatenate([y_global, y_samples])
                cov_global = nancov(y_global)
                # write global covariance matrix
                outpath = outdir[scale].incorporate(
                    os.path.basename(path).split("_", maxsplit=1)[0] +
                    "_global.cov")
                print(
                    "writing covariance matrix to:     %s" %
                    os.path.basename(outpath))
                header = "correlation amplitude (%s) covariance matrix" % key
                np.savetxt(outpath, cov_global, header=header)
                # write the order for later use
                with open(os.path.splitext(outpath)[0] + ".order", "w") as f:
                    for zkey in args.cov_order:
                        f.write("%s\n" % zkey)
