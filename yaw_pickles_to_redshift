#!/usr/bin/env python3
import argparse
import os
import warnings
from collections import Counter
from hashlib import md5

import numpy as np
from numpy import ma
from yaw_tools.folders import (
    binname, CCFolder, DEFAULT_EXT_DATA, DEFAULT_EXT_BOOT, DEFAULT_EXT_COV)
from yaw_tools.pickles import AutoCorrelationPickle, CrossCorrelationPickle


def check_autocorrelation(scaledir, suffix_name, name):
    ac_dict = scaledir.list_autocorr_files(".pkl")
    if suffix_name is not None:
        try:
            pickle_path = ac_dict[suffix_name]
            print("found %s auto-correlation pickle" % name)
        except KeyError:
            raise ValueError("%s auto-correlation pickle not found" % name)
    else:
        pickle_path = None
    use_corr = pickle_path is not None
    return pickle_path, use_corr


def nancov(bootstraps):
    # mask infinite values
    mask = np.logical_not(np.isfinite(bootstraps))
    masked_boots = ma.array(bootstraps, mask=mask)
    # compute covariance
    covar = ma.cov(masked_boots, ddof=0)
    # find rows/columns with NaNs
    diag = np.diag(covar)
    idx = np.arange(len(diag))
    idx = idx[np.isnan(diag)]
    # set the diag. element to infinity and the off-diag elements to 0
    for i in idx:
        covar[i, :] = 0.0
        covar[:, i] = 0.0
        covar[i, i] = np.inf
    return covar


def convert_pickle(
        scaledir, pickle_path, header_key, bias=None, bias_samples=None):
    pickler = AutoCorrelationPickle(pickle_path)
    # initialize the bootstrap resampling indices
    global boot_idx
    if boot_idx is None:
        pickler.generate_sampling_idx(n_bootstraps=args.n_boot)
        boot_idx = cc.get_sampling_idx()
    else:
        pickler.set_sampling_idx(boot_idx)
    # create the realisations
    redshifts = pickler.get_redshifts()
    amplitudes = pickler.get_amplitudes()
    samples = pickler.get_samples()
    if bias is None:
        errors = pickler.get_errors()
    else:
        # compute errors of bias corrected correlation amplitudes
        errors = np.nanstd(samples[zbin], axis=1)
    # create correlation amplitude file
    outpath = scaledir.incorporate(pickle_path, DEFAULT_EXT_DATA)
    print(
        "writing data to: %s.*" %
        os.path.basename(os.path.splitext(outpath)[0]))
    nz_array = np.transpose([redshifts, amplitudes, errors])
    header = "col 1 = mean redshift\n"
    header += "col 2 = correlation amplitude (%s)\n" % key
    header += "col 3 = amplitude error"
    np.savetxt(outpath, nz_array, header=header)
    # store optional bootstrap samples
    if args.store_boot:
        outpath = scaledir.incorporate(pickle_path, DEFAULT_EXT_DATA)
        np.savetxt(
            outpath, samples,
            header="correlation amplitude (%s) realisations" % header_key)
    # store covariance matrix
    if args.store_cov:
        outpath = scaledir.incorporate(pickle_path, DEFAULT_EXT_COV)
        np.savetxt(
            outpath, nancov(samples),
            header="correlation amplitude (%s) covariance matrix" % header_key)
    # return data needed for CC bias mitigation
    return amplitudes, samples


parser = argparse.ArgumentParser(
    description='Unpickles yet_another_wizz region pickle files, optionally '
                'applying bias corrections.')
parser.add_argument(
    'wdir', metavar='DATAFOLDER',
    help='an output folder of yet_another_wizz')
parser.add_argument(
    '--bias-spec',
    help='suffix of the auto-correlation files to apply for correcting the '
         'spectroscopic bias (optional)')
parser.add_argument(
    '--bias-phot',
    help='suffix of the auto-correlation files to apply for correcting the '
         'photometric bias (optional)')
parser.add_argument(
    '--seed',
    help='string from which a seed for the random generator is computed')
parser.add_argument(
    '--n-boot', type=int, default=1000,
    help='number of bootstrap realisations for covariance estimation')
parser.add_argument(
    '--store-boot', action='store_true',
    help='save the bootstrap realisations in separate files')
parser.add_argument(
    '--cov-order', nargs='*',
    help='order of redshift keys (e.g. 0.101z1.201) in which the cross-'
         'correlation files are inserted into the global covariance matrix, '
         'if not given no global covariance is computed')
parser.add_argument(
    '--store-cov', action='store_true',
    help='save covariance matrices in separate files')
parser.add_argument(
    '-o', '--output',
    help='folder in which the output is stored (optional)')


if __name__ == "__main__":

    args = parser.parse_args()

    setattr(args, "wdir", os.path.abspath(os.path.expanduser(args.wdir)))
    if not os.path.exists(args.wdir):
        raise OSError("input folder does not exist")
    print("==> processing data in: %s" % args.wdir)
    indir = CCFolder(args.wdir)

    # check if a binning file and which scale dirs exits
    if not os.path.exists(indir.path_binning_file()):
        raise ValueError("input folder does not contain valid output")

    print("finding correlation scales")
    scales = set(indir.list_scalenames())
    if len(scales) == 0:
        raise ValueError("input folder contains no scales")
    print("found scales: %s" % set(os.path.basename(s) for s in scales))

    # initialize the random state
    hasher = md5(bytes(args.seed, "utf-8"))
    hashval = bytes(hasher.hexdigest(), "utf-8")
    np.random.seed(np.frombuffer(hashval, dtype=np.uint32))
    # global should be always the same once initialized
    boot_idx = None

    # convert the pickles
    for scale, scaledir in indir.iter_scales():
        print("==> processing scale: %s" % scale)

        # check which correlation pickles exist
        cc_pickles = scaledir.list_crosscorr_files(".pkl")
        if len(cc_files) == 0:
            raise ValueError("no cross-correlation pickles found")
        # figure out whether to use spectroscopic auto-correlation
        ac_spec_pickle, use_wss = check_autocorrelation(
            scaledir, args.bias_spec, "spec.")
        # figure out whether to use photometric auto-correlation
        ac_phot_pickle, use_wpp = check_autocorrelation(
            scaledir, args.bias_phot, "phot.")

        # now everything should be good and we can produce output
        if args.output is None:
            outdir = indir
        else:
            outdir = indir.copy_meta_to(args.output)

        # NaNs are to be expected so we ignore warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")

            # process auto-correlation pickles and compute the bias correction
            amp_product = np.ones_like(amplitudes)
            amp_product_samples = np.ones_like(y_samples)
            header_key = "w_sp"
            if use_wss:
                amplitudes, samples = convert_pickle(
                    scaledir, ac_spec_pickle, "w_ss")
                amp_product *= amplitudes
                amp_product_samples *= samples
                key += " / bias_spec"
            if use_wpp:
                amplitudes, samples = convert_pickle(
                    scaledir, ac_phot_pickle, "w_pp")
                amp_product *= amplitudes
                amp_product_samples *= samples
                key += " / bias_phot"
            # apply the bias correction
            bias = np.sqrt(amp_product)
            bias_samples = np.sqrt(amp_product_samples)
            

            # process the cross-correlation pickles
            samples = {}  # needed for correlation between bins
            for zbin, pickle_path in cc_pickles.items():
                amplitudes, samples[zbin] convert_pickle(
                    scaledir, pickle_path, header_key,
                    bias=bias, bias_samples=bias_samples)

            # compute global covariance matrix
            if args.cov_order is not None and args.store_cov:
                data_keys = samples.keys()
                global_samples = None
                for zbin in args.cov_order:
                    try:
                        if global_samples is None:
                            global_samples = samples[zbin]
                        else:
                            global_samples = np.concatenate([
                                global_samples, samples[zbin]])
                    except KeyError:
                        string = "redshift bin '%s' from --cov-order " % zbin
                        string += "not found"
                        raise KeyError(string)
                # store global covariance matrix
                outpath = scaledir.incorporate(
                    os.path.basename(pickle_path).split("_")[0] +
                    "_global%s" % DEFAULT_EXT_COV)
                print(
                    "writing global covariance matrix to: %s" %
                    os.path.basename(outpath))
                header = "global correlation amplitude "
                header += "(%s) covariance matrix" % header_key
                np.savetxt(outpath, nancov(global_samples), header=header)
                # store the order for later use
                with open(os.path.splitext(outpath)[0] + ".order", "w") as f:
                    for zbin in args.cov_order:
                        f.write("%s\n" % zkey)
            elif args.cov_order is None and args.store_cov:
                print("omitting global covariance estimation")
