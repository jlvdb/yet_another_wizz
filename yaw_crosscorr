#!/usr/bin/env python3
import argparse
import pickle
import os
import shutil
import sys
from multiprocessing import cpu_count

import numpy as np
import pandas as pd
from astropy.io import fits as pyfits
from yaw_tools.correlation import bin_table, get_bin_weights
from yaw_tools.folders import CCFolder, Folder
from yaw_tools.utils import (DEFAULT_PAIR_WEIGHTING, DEFAULT_CAT_EXT,
                             DEFAULT_REGION_NO, DEFAULT_RESAMPLING)
from yet_another_wizz.PairMaker import PairMaker
from yet_another_wizz.PdfMaker import PdfMaker
from yet_another_wizz.utils import write_argparse_summary


parser = argparse.ArgumentParser(
    description='Wrapper for pair_maker.py provided by the-wizz to mask the '
                'data and compute the cross-correlation between the '
                'photometric and the spectroscopic samples.')
parser.add_argument(
    'wdir', metavar='OUTPUT', help='folder in which all output is stored')
parser.add_argument(
    '--exit-on-warning', action='store_true',
    help='handle warnings as exceptions')
parser.add_argument(
    '--threads', type=int, default=cpu_count(),
    help='number of threads to use (default: %(default)s)')

ref_cat = parser.add_argument_group(
    title="reference sample catalogue file",
    description="defining FITS file and table data columns")
ref_cat.add_argument(
    '--ref-file', metavar='FILE', required=True,
    help='spectroscopic catalogue file path')
ref_cat.add_argument(
    '--ref-file-ext', metavar='INT', type=int, default=DEFAULT_CAT_EXT,
    help='fits extesion containg data (default: %(default)s)')
ref_cat.add_argument(
    '--ref-ra', metavar='FIELD', required=True,
    help='right ascension column name')
ref_cat.add_argument(
    '--ref-dec', metavar='FIELD', required=True,
    help='declination column name')
ref_cat.add_argument(
    '--ref-z', metavar='FIELD', required=True,
    help='spectroscopic (known) redshift column name')
ref_cat.add_argument(
    '--ref-idx', metavar='FIELD', help='object index column name')
ref_cat.add_argument(
    '--ref-weight', metavar='FIELD', help='object weight column name')

test_cat = parser.add_argument_group(
    title="test sample catalogue file",
    description="defining FITS file and table data columns")
test_cat.add_argument(
    '--test-file', metavar='FILE', required=True,
    help='photometric catalogue file path')
test_cat.add_argument(
    '--test-file-ext', metavar='INT', type=int, default=DEFAULT_CAT_EXT,
    help='fits extesion containg data (default: %(default)s)')
test_cat.add_argument(
    '--test-ra', metavar='FIELD', required=True,
    help='right ascension column name')
test_cat.add_argument(
    '--test-dec', metavar='FIELD', required=True,
    help='declination column name')
test_cat.add_argument(
    '--test-z', metavar='FIELD',
    help='photometric (estimated) redshift column name')
test_cat.add_argument(
    '--test-weight', metavar='FIELD', help='object weight column name')

rand_cat = parser.add_argument_group(
    title="random sample catalogue file",
    description="defining FITS file and table data columns")
rand_cat.add_argument(
    '--rand-file', metavar='FILE', required=True,
    help='random catalogue file path')
rand_cat.add_argument(
    '--rand-file-ext', metavar='INT', type=int, default=DEFAULT_CAT_EXT,
    help='fits extesion containg data')
rand_cat.add_argument(
    '--rand-ra', metavar='FIELD', required=True,
    help='right ascension column name')
rand_cat.add_argument(
    '--rand-dec', metavar='FIELD', required=True,
    help='declination column name')
rand_cat.add_argument(
    '--rand-z', metavar='FIELD',
    help='photometric (estimated) redshift column name')
rand_cat.add_argument(
    '--rand-weight', metavar='FIELD', help='object weight column name')

analysis_parameters = parser.add_argument_group(
    title='analysis parameters',
    description='controlling the autocorrelation computation')
analysis_parameters.add_argument(
    '--scales-min', metavar='LIST', default='100',
    help='comma separated list of inner radii of analysis annulus '
         'on sky in kpc (default: %(default)s)')
analysis_parameters.add_argument(
    '--scales-max', metavar='LIST', default='1000',
    help='comma separated list of outer radii of analysis annulus '
         'on sky in kpc (default: %(default)s)')
analysis_parameters.add_argument(
    '--across-regions', metavar='Y/N', default='N',
    help='wether to include pairs with galaxies from neighboring spatial '
         'regions')
analysis_parameters.add_argument(
    '--z-min', metavar='FLOAT', type=float, default=0.01,
    help='minimum analysis redshift (default: %(default)s)')
analysis_parameters.add_argument(
    '--z-max', metavar='FLOAT', type=float, default=5.01,
    help='maximum analysis redshift (default: %(default)s)')
analysis_parameters.add_argument(
    '--resampling', metavar='INT', type=int, default=DEFAULT_RESAMPLING,
    help='number of random galaxy realizations created (default: %(default)s)')
analysis_parameters.add_argument(
    '--ref-source-limit', metavar='INT', type=int, default=10,
    help='minimum number of sources in the reference catalogue after masking')
analysis_parameters.add_argument(
    '--ref-bin-no', metavar='INT', type=int, default=20,
    help='number of spectroscopic bins (default: %(default)s)')
analysis_parameters.add_argument(
    '--ref-bin-type', default="comoving", nargs='*',
    # choices=["linear", "adapt", "comoving", "logspace"],
    help='spacing of the spectroscopic bins (default: %(default)s)')
analysis_parameters.add_argument(
    '--test-bin-edges', metavar='LIST',
    help='bin edges of the photometric redshift preselection (default: '
         'disabled), comma seaprated values define contiguous bins edges, '
         'colon separated values define non-contiguous bin edges -- '
         'e.g.: "0.1,0.3,0.5;0.1,0.5" creates bins 0.1 to 0.3, 0.3 to 0.5 and '
         '0.1 to 0.5')
analysis_parameters.add_argument(
    '--pair-weighting', metavar='Y/N', default=DEFAULT_PAIR_WEIGHTING,
    help='wether to weight pairs by the inverse separation')
analysis_parameters.add_argument(
    '--R-D-ratio', default="local",
    help='ratio of random to data objects (default: local)')


if __name__ == '__main__':

    args = parser.parse_args()

    args.map_region_no = 1  # compatibilty

    # check z_min:
    if args.z_min <= 0.0:
        raise ValueError("--z-min must be larger than zero")

    # make working dir if not existing
    setattr(args, "wdir", os.path.abspath(os.path.expanduser(args.wdir)))
    sys.stdout.write("set output folder to %s\n" % args.wdir)
    sys.stdout.flush()
    outdir = CCFolder(args.wdir)

    # write input parameter summary
    for arg in ("ref_file", "test_file", "rand_file"):
        path = getattr(args, arg)
        if path is not None:
            setattr(args, arg, os.path.abspath(os.path.expanduser(path)))
    write_argparse_summary(args, outdir.path_params_file())

    # load the reference sample
    print("==> loading reference objects")
    with pyfits.open(args.ref_file) as fits:
        ext = args.ref_file_ext
        refdata = pd.DataFrame({
            "RA": fits[ext].data[args.ref_ra].byteswap().newbyteorder(),
            "DEC": fits[ext].data[args.ref_dec].byteswap().newbyteorder(),
            "z": fits[ext].data[args.ref_z].byteswap().newbyteorder()})
        if args.ref_weight is not None:
            refdata["weight"] = \
                fits[1].data[args.ref_weight].byteswap().newbyteorder()
    if len(refdata) < args.ref_source_limit:
        if args.exit_on_warning:
            print(
                "ERROR: there are less then %d" % args.ref_source_limit +
                " reference sources after masking")
            print("remove all output")
            shutil.rmtree(outdir.root)
            sys.exit(1)
        else:
            sys.stdout.write(
                "WARNING: there are less then %d" % args.ref_source_limit +
                " reference sources after masking\n")
        sys.stdout.flush()

    # create tomographic bins
    testbindir = Folder(outdir.join("test_sample_bins"))
    randbindir = Folder(outdir.join("random_sample_bins"))
    if args.test_bin_edges is not None:
        # construct tomographic bins
        zbins = []
        for zlist in args.test_bin_edges.split(";"):
            edges = zlist.split(",")
            if len(edges) < 2:
                raise ValueError(
                    "invalid format in bin edges: " + args.test_bin_edges)
            for i in range(len(edges) - 1):
                zbins.append([float(edges[i]), float(edges[i + 1])])
        # load and bin the test and random data
        print("==> loading unknown objects")
        testdata, testfiles = bin_table(
            testbindir, args.test_file, args.test_ra, args.test_dec,
            args.test_z, args.test_weight,
            zbins=zbins, cat_ext=args.test_file_ext)
        print("==> loading random objects")
        randdata, randfiles = bin_table(
            randbindir, args.rand_file, args.rand_ra, args.rand_dec,
            args.rand_z, args.rand_weight,
            zbins=zbins, cat_ext=args.rand_file_ext)
        bin_weights_dict = get_bin_weights(testdata, testfiles)
    else:
        print("==> loading unknown objects")
        testdata, testfiles = bin_table(
            testbindir, args.test_file, args.test_ra, args.test_dec,
            args.test_z, args.test_weight,
            zbins=None, cat_ext=args.test_file_ext)
        print("==> loading random objects")
        randdata, randfiles = bin_table(
            randbindir, args.rand_file, args.rand_ra, args.rand_dec,
            args.rand_z, args.rand_weight,
            zbins=None, cat_ext=args.rand_file_ext)
        bin_weights_dict = None

    try:
        D_R_ratio = 1.0 / float(args.R_D_ratio)
    except ValueError:
        D_R_ratio = args.R_D_ratio

    rlimits = [
        (float(rmin), float(rmax)) for rmin, rmax in
        zip(args.scales_min.split(","), args.scales_max.split(","))]
    scale_names = [outdir.add_scale(rlim) for rlim in rlimits]
    # write bin weight pickles for each scale
    for scale in scale_names:
        with open(outdir[scale].path_weights_file(), "wb") as f:
            pickle.dump(bin_weights_dict, f)


    sys.stdout.write("==> run yet another wizz\n")
    sys.stdout.flush()

    # iterate the binned test data and randoms
    for i in range(len(testdata)):

        redshift = os.path.splitext(os.path.basename(testfiles[i]))[0]
        redshift = redshift.rsplit("_", 1)[-1]
        sys.stdout.write("\n==> processing redshift slice %s\n" % redshift)
        sys.stdout.flush()

        # run YAW
        est = PairMaker(threads=args.threads, verbose=True)
        try:
            est.setUnknown(
                testdata[i].RA, testdata[i].DEC, testdata[i].z,
                testdata[i].weight)
        except AttributeError:
            try:
                est.setUnknown(
                    testdata[i].RA, testdata[i].DEC, testdata[i].z)
            except AttributeError:
                    est.setUnknown(testdata[i].RA, testdata[i].DEC)
        try:
            est.setRandoms(
                randdata[i].RA, randdata[i].DEC, randdata[i].z,
                randdata[i].weight)
        except AttributeError:
            try:
                est.setRandoms(
                    randdata[i].RA, randdata[i].DEC, randdata[i].z)
            except AttributeError:
                est.setRandoms(randdata[i].RA, randdata[i].DEC)
        if "weight" in refdata:
            est.setReference(
                refdata.RA, refdata.DEC, refdata.z, refdata.weight)
        else:
            est.setReference(refdata.RA, refdata.DEC, refdata.z, None)

        for scale_name, (rmin, rmax) in zip(scale_names, rlimits):
            scaledir = outdir[scale_name]
            sys.stdout.write("==> running scale %s\n" % scale_name)
            sys.stdout.flush()
            # file names of intermediate products
            paircoutdir = Folder(outdir.join("paircounts_" + scale_name))
            if args.test_bin_edges is None:
                meta_file = paircoutdir.join("CC_meta.pkl")
                paircount_file = paircoutdir.join("CC.pqt")
            else:
                meta_file = paircoutdir.join("CC_%s_meta.pkl" % redshift)
                paircount_file = paircoutdir.join("CC_%s.pqt" % redshift)
            # count pairs
            est.countPairs(
                rmin=rmin, rmax=rmax, comoving=False,
                inv_distance_weight=(args.pair_weighting == "Y"),
                D_R_ratio=D_R_ratio,
                regionize_unknown=(args.across_regions == "N"))
            est.writeMeta(meta_file)
            est.writeCounts(paircount_file)
            # create region pickle
            regionpicklefile = scaledir.path_crosscorr_file(
                ".pkl", zlims=None if redshift == "all" else redshift)
            try:
                pdf = PdfMaker(paircount_file, autocorr=False)
                if args.ref_bin_type[0] == "adapt":
                    raise NotImplementedError()
                else:
                    pdf.generateBinning(
                        args.ref_bin_no, args.z_min, args.z_max,
                        args.ref_bin_type[0])
                pdf.writeBinning(outdir.path_binning_file())
                pdf.writeRegionPickle(regionpicklefile)
            except ValueError:  # empty pair counts file
                print(
                    "WARNING: the pair count file was empty, create dummy "
                    "output instead")
                array_shape = (args.ref_bin_no, args.map_region_no)
                region_pickle = {
                    "n_reference": np.zeros(array_shape, dtype=np.int_),
                    "redshift": np.zeros(array_shape, dtype=np.float_),
                    "unknown": np.zeros(array_shape, dtype=np.float_),
                    "rand": np.zeros(array_shape, dtype=np.float_),
                    "n_regions": args.map_region_no}
                print(
                    "writing dummy region pickle to:\n    %s" %
                    regionpicklefile)
                with open(regionpicklefile, "wb") as f:
                    pickle.dump(region_pickle, f)
