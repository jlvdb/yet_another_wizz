#!/usr/bin/env python3
import argparse
import os
import shutil
import sys
from multiprocessing import cpu_count

import numpy as np
import pandas as pd
from astropy.io import fits as pyfits
from yaw_tools.correlation import (bin_table, get_bin_weights,
                                   write_argparse_summary)
from yaw_tools.folders import CCFolder, Folder
from yaw_tools.region_counts import get_region_number
from yaw_tools.utils import (DEFAULT_PAIR_WEIGHTING, DEFAULT_CAT_EXT,
                             DEFAULT_RESAMPLING)
from yet_another_wizz.PairMaker import PairMaker
from yet_another_wizz.PdfMaker import PdfMaker
from yet_another_wizz.utils import dump_json, NAMED_COSMOLOGIES


parser = argparse.ArgumentParser(
    description='Wrapper for pair_maker.py provided by the-wizz to mask the '
                'data and compute the cross-correlation between the '
                'photometric and the spectroscopic samples.')
parser.add_argument(
    'wdir', metavar='OUTPUT', help='folder in which all output is stored')
parser.add_argument(
    '--exit-on-warning', action='store_true',
    help='handle warnings as exceptions')
parser.add_argument(
    '--threads', type=int, default=cpu_count(),
    help='number of threads to use (default: %(default)s)')

ref_cat = parser.add_argument_group(
    title="reference sample catalogue file",
    description="defining FITS file and table data columns")
ref_cat.add_argument(
    '--ref-file', metavar='FILE', required=True,
    help='spectroscopic catalogue file path')
ref_cat.add_argument(
    '--ref-file-ext', metavar='INT', type=int, default=DEFAULT_CAT_EXT,
    help='fits extesion containg data (default: %(default)s)')
ref_cat.add_argument(
    '--ref-ra', metavar='FIELD', required=True,
    help='right ascension column name')
ref_cat.add_argument(
    '--ref-dec', metavar='FIELD', required=True,
    help='declination column name')
ref_cat.add_argument(
    '--ref-z', metavar='FIELD', required=True,
    help='spectroscopic (known) redshift column name')
ref_cat.add_argument(
    '--ref-weight', metavar='FIELD', help='object weight column name')
ref_cat.add_argument(
    '--ref-region', metavar='FIELD', help='spatial region index column name')

test_cat = parser.add_argument_group(
    title="test sample catalogue file",
    description="defining FITS file and table data columns")
test_cat.add_argument(
    '--test-file', metavar='FILE', required=True,
    help='photometric catalogue file path')
test_cat.add_argument(
    '--test-file-ext', metavar='INT', type=int, default=DEFAULT_CAT_EXT,
    help='fits extesion containg data (default: %(default)s)')
test_cat.add_argument(
    '--test-ra', metavar='FIELD', required=True,
    help='right ascension column name')
test_cat.add_argument(
    '--test-dec', metavar='FIELD', required=True,
    help='declination column name')
test_cat.add_argument(
    '--test-z', metavar='FIELD',
    help='photometric (estimated) redshift column name')
test_cat.add_argument(
    '--test-weight', metavar='FIELD', help='object weight column name')
test_cat.add_argument(
    '--test-region', metavar='FIELD', help='spatial region index column name')

rand_cat = parser.add_argument_group(
    title="random sample catalogue file",
    description="defining FITS file and table data columns")
rand_cat.add_argument(
    '--rand-file', metavar='FILE', required=True,
    help='random catalogue file path')
rand_cat.add_argument(
    '--rand-file-ext', metavar='INT', type=int, default=DEFAULT_CAT_EXT,
    help='fits extesion containg data')
rand_cat.add_argument(
    '--rand-ra', metavar='FIELD', required=True,
    help='right ascension column name')
rand_cat.add_argument(
    '--rand-dec', metavar='FIELD', required=True,
    help='declination column name')
rand_cat.add_argument(
    '--rand-z', metavar='FIELD',
    help='photometric (estimated) redshift column name')
rand_cat.add_argument(
    '--rand-weight', metavar='FIELD', help='object weight column name')
rand_cat.add_argument(
    '--rand-region', metavar='FIELD', help='spatial region index column name')

analysis_parameters = parser.add_argument_group(
    title='analysis parameters',
    description='controlling the autocorrelation computation')
analysis_parameters.add_argument(
    '--cosmology', metavar='STR', choices=NAMED_COSMOLOGIES, default='default',
    help='astropy cosmology to use for distance calculations '
         '(default: astropy default)')
analysis_parameters.add_argument(
    '--scales-min', metavar='LIST', default='100',
    help='comma separated list of inner radii of analysis annulus '
         'on sky in kpc (default: %(default)s)')
analysis_parameters.add_argument(
    '--scales-max', metavar='LIST', default='1000',
    help='comma separated list of outer radii of analysis annulus '
         'on sky in kpc (default: %(default)s)')
analysis_parameters.add_argument(
    '--across-regions', metavar='Y/N', default='N',
    help='wether to include pairs with galaxies from neighboring spatial '
         'regions')
analysis_parameters.add_argument(
    '--z-min', metavar='FLOAT', type=float, default=0.01,
    help='minimum analysis redshift (default: %(default)s)')
analysis_parameters.add_argument(
    '--z-max', metavar='FLOAT', type=float, default=5.01,
    help='maximum analysis redshift (default: %(default)s)')
analysis_parameters.add_argument(
    '--resampling', metavar='INT', type=int, default=DEFAULT_RESAMPLING,
    help='number of random galaxy realizations created (default: %(default)s)')
analysis_parameters.add_argument(
    '--ref-source-limit', metavar='INT', type=int, default=10,
    help='minimum number of sources in the reference catalogue after masking')
analysis_parameters.add_argument(
    '--ref-bin-no', metavar='INT', type=int, default=20,
    help='number of spectroscopic bins (default: %(default)s)')
analysis_parameters.add_argument(
    '--ref-bin-type', default="comoving", nargs='*',
    choices=["linear", "adapt", "comoving", "logspace"],
    help='spacing of the spectroscopic bins (default: %(default)s)')
analysis_parameters.add_argument(
    '--test-bin-edges', metavar='LIST',
    help='bin edges of the photometric redshift preselection (default: '
         'disabled), comma seaprated values define contiguous bins edges, '
         'colon separated values define non-contiguous bin edges -- '
         'e.g.: "0.1,0.3,0.5;0.1,0.5" creates bins 0.1 to 0.3, 0.3 to 0.5 and '
         '0.1 to 0.5')
analysis_parameters.add_argument(
    '--pair-weighting', metavar='Y/N', default=DEFAULT_PAIR_WEIGHTING,
    help='wether to weight pairs by the inverse separation')
analysis_parameters.add_argument(
    '--R-D-ratio', default="local",
    help='ratio of random to data objects (default: local)')


if __name__ == '__main__':

    args = parser.parse_args()

    # check region column arguments
    regions_set = [
        val is not None for val in (
            args.ref_region, args.test_region, args.rand_region)]
    if any(regions_set) and not all(regions_set):
        raise parser.error(
            "if region indices (--*-region) are used for one data set they "
            "are required for all data sets")

    # check z_min:
    if args.z_min <= 0.0:
        raise ValueError("--z-min must be larger than zero")

    # make working dir if not existing
    setattr(args, "wdir", os.path.abspath(os.path.expanduser(args.wdir)))
    sys.stdout.write("set output folder to %s\n" % args.wdir)
    sys.stdout.flush()
    outdir = CCFolder(args.wdir)

    # write input parameter summary
    for arg in ("ref_file", "test_file", "rand_file"):
        path = getattr(args, arg)
        if path is not None:
            setattr(args, arg, os.path.abspath(os.path.expanduser(path)))
    write_argparse_summary(args, outdir.path_params_file())

    # load the reference sample
    print("==> loading reference objects")
    with pyfits.open(args.ref_file) as fits:
        data = fits[args.ref_file_ext].data
        refdata = pd.DataFrame({
            "RA": data[args.ref_ra].byteswap().newbyteorder(),
            "DEC": data[args.ref_dec].byteswap().newbyteorder(),
            "Z": data[args.ref_z].byteswap().newbyteorder()})
        if args.ref_weight is not None:
            refdata["weights"] = \
                data[args.ref_weight].byteswap().newbyteorder()
        if args.ref_region is not None:
            endian_corrected = data[args.ref_region].byteswap().newbyteorder()
            refdata["region_idx"] = endian_corrected.astype(np.int16)
    if len(refdata) < args.ref_source_limit:
        if args.exit_on_warning:
            print(
                "ERROR: there are less then %d" % args.ref_source_limit +
                " reference sources after masking")
            print("remove all output")
            shutil.rmtree(outdir.root)
            sys.exit(1)
        else:
            sys.stdout.write(
                "WARNING: there are less then %d" % args.ref_source_limit +
                " reference sources after masking\n")
        sys.stdout.flush()

    # create tomographic bins
    testbindir = Folder(outdir.join("test_sample_bins"))
    randbindir = Folder(outdir.join("random_sample_bins"))
    if args.test_bin_edges is not None:
        # construct tomographic bins
        zbins = []
        for zlist in args.test_bin_edges.split(";"):
            edges = zlist.split(",")
            if len(edges) < 2:
                raise ValueError(
                    "invalid format in bin edges: " + args.test_bin_edges)
            for i in range(len(edges) - 1):
                zbins.append([float(edges[i]), float(edges[i + 1])])
    else:
        zbins = None
    # load and bin the test and random data
    print("==> loading unknown objects")
    testdata, testfiles = bin_table(
        testbindir, args.test_file, args.test_ra, args.test_dec,
        args.test_z, args.test_weight, args.test_region,
        zbins=zbins, cat_ext=args.test_file_ext)
    print("==> loading random objects")
    randdata, randfiles = bin_table(
        randbindir, args.rand_file, args.rand_ra, args.rand_dec,
        args.rand_z, args.rand_weight, args.rand_region,
        zbins=zbins, cat_ext=args.rand_file_ext)
    # figure out the bin weights
    bin_weights_dict = get_bin_weights(testdata, testfiles)
    # get the number of spatial regions from the random data region indices
    n_regions = get_region_number(*randdata)

    # configure the random normalisation
    try:
        D_R_ratio = 1.0 / float(args.R_D_ratio)
    except ValueError:
        D_R_ratio = args.R_D_ratio

    rlimits = [
        (float(rmin), float(rmax)) for rmin, rmax in
        zip(args.scales_min.split(","), args.scales_max.split(","))]
    scale_names = [outdir.add_scale(rlim) for rlim in rlimits]
    # write bin weight counts for each scale
    for scale in scale_names:
        dump_json(bin_weights_dict, outdir[scale].path_weights_file())

    sys.stdout.write("\n==> run yet another wizz\n")
    sys.stdout.flush()

    # iterate the binned test data and randoms
    for i in range(len(testdata)):

        redshift = os.path.splitext(os.path.basename(testfiles[i]))[0]
        redshift = redshift.rsplit("_", 1)[-1]
        sys.stdout.write("\n==> processing redshift slice %s\n" % redshift)
        sys.stdout.flush()

        # run YAW
        est = PairMaker(threads=args.threads, verbose=True)
        est.setCosmology(args.cosmology)
        est.setRandoms(**randdata[i])
        est.setUnknown(**testdata[i])
        est.setReference(**refdata)
        # write the used cosmology
        cosmo_file = outdir.path_cosmology_file()
        if not os.path.exists(cosmo_file):
            est.writeCosmology(cosmo_file)

        # process scale after scale
        for scale_name, (rmin, rmax) in zip(scale_names, rlimits):
            scaledir = outdir[scale_name]
            sys.stdout.write("==> running scale %s\n" % scale_name)
            sys.stdout.flush()
            # file names of intermediate products
            paircoutdir = Folder(outdir.join("paircounts_" + scale_name))
            if args.test_bin_edges is None:
                meta_file = paircoutdir.join("CC_meta.json")
                paircount_file = paircoutdir.join("CC.pqt")
            else:
                meta_file = paircoutdir.join("CC_%s_meta.json" % redshift)
                paircount_file = paircoutdir.join("CC_%s.pqt" % redshift)

            # count pairs
            est.countPairs(
                rmin=rmin, rmax=rmax, comoving=False,
                inv_distance_weight=(args.pair_weighting == "Y"),
                D_R_ratio=D_R_ratio,
                regionize_unknown=(args.across_regions == "N"))
            est.writeMeta(meta_file)
            est.writeCounts(paircount_file)

            # create region counts
            regioncountsfile = scaledir.path_crosscorr_file(
                ".json", zlims=None if redshift == "all" else redshift)
            try:
                pdf = PdfMaker(paircount_file, autocorr=False)
                pdf.setCosmology(args.cosmology)
                if args.ref_bin_type[0] == "adapt":
                    raise NotImplementedError()
                else:
                    pdf.generateBinning(
                        args.ref_bin_no, args.z_min, args.z_max,
                        args.ref_bin_type[0])
                pdf.writeBinning(outdir.path_binning_file())
                pdf.writeRegionDict(regioncountsfile)
            # empty pair counts file
            except ValueError:
                print(
                    "WARNING: the pair count file was empty, create dummy "
                    "output instead")
                array_shape = (args.ref_bin_no, n_regions)
                region_counts = {
                    "n_reference": np.zeros(array_shape, dtype=np.int_),
                    "sum_redshifts": np.zeros(array_shape, dtype=np.float_),
                    "data_data": np.zeros(array_shape, dtype=np.float_),
                    "data_random": np.zeros(array_shape, dtype=np.float_),
                    "n_regions": n_regions}
                print(
                    "writing dummy region counts to:\n    %s" %
                    regioncountsfile)
                dump_json(region_counts, regioncountsfile)
