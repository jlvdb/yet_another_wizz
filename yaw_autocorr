#!/usr/bin/env python3
import argparse
import copy
import os
import pickle
import shutil
import sys
from multiprocessing import cpu_count

import numpy as np
import pandas as pd
from astropy.cosmology import default_cosmology
from astropy.io import fits as pyfits
from yaw_tools.correlation import run_ac_single_bin
from yaw_tools.folders import CCFolder, Folder
from yaw_tools.utils import (DEFALUT_PAIR_WEIGHTING, DEFAULT_CAT_EXT,
                             DEFAULT_REGION_NO, DEFAULT_RESAMPLING, TypeNone)
from yet_another_wizz.PairMaker import PairMaker
from yet_another_wizz.PdfMaker import PdfMaker
from yet_another_wizz.utils import ThreadHelper, load_argparse_summary


parser = argparse.ArgumentParser(
    description='Compute an estimate for the galaxy bias of a spectroscopic '
                'data sample by measuring the autocorrelation amplitude at '
                'the same comoving projected radii used for the cross-'
                'correlation measurement with the-wizz.'
                '(Rahman et. al. 2016)')

parser.add_argument(
    'wdir', metavar='DATAFOLDER',
    help='folder in which the output is stored')
parser.add_argument(
    '--binning-file', metavar='FILE', required=True,
    help='path to a custom file defining the redshift bin edges')
parser.add_argument(
    '--which', choices=("ref", "test"), required=True,
    help='which type of autocorrelation to compute')
parser.add_argument(
    '--param-file', metavar='FILE',
    help='file with parameters from a yet_another_wizz run to complete '
         'omitted input')
parser.add_argument(
    '--threads', metavar='INT', type=int)
parser.add_argument(
    '--output-suffix', metavar='str',
    help='suffix to append to the auto-correlation and region pickle files')

ref_cat = parser.add_argument_group(
    title="data sample catalogue file",
    description="defining FITS file and table data columns")
ref_cat.add_argument(
    '--cat-file', metavar='FILE', help='data catalogue file path')
ref_cat.add_argument(
    '--cat-file-ext', metavar='INT', type=TypeNone(int),
    help='fits extesion containg data (default: %d)' % DEFAULT_CAT_EXT)
ref_cat.add_argument(
    '--cat-ra', metavar='FIELD',
    help='right ascension column name')
ref_cat.add_argument(
    '--cat-dec', metavar='FIELD', help='declination column name')
ref_cat.add_argument(
    '--cat-z', metavar='FIELD', help='redshift column name')
ref_cat.add_argument(
    '--cat-weight', metavar='FIELD',
    help='object weight column name')

rand_cat = parser.add_argument_group(
    title="random sample catalogue file",
    description="defining FITS file and table data columns")
rand_cat.add_argument(
    '--rand-file', metavar='FILE',
    help='random catalogue file path')
rand_cat.add_argument(
    '--rand-file-ext', metavar='INT', type=TypeNone(int),
    help='fits extesion containg data (default: %d)' % DEFAULT_CAT_EXT)
rand_cat.add_argument(
    '--rand-ra', metavar='FIELD',
    help='right ascension column name')
rand_cat.add_argument(
    '--rand-dec', metavar='FIELD',
    help='declination column name')
rand_cat.add_argument(
    '--rand-z', metavar='FIELD',
    help='photometric (estimated) redshift column name')
rand_cat.add_argument(
    '--rand-weight', metavar='FIELD',
    help='object weight column name, WARNING: these are not passed on '
         'automatically')

stomp_map = parser.add_argument_group(
    title='stomp map',
    description='defining the mask and regions for spatial bootstrapping')
stomp_map.add_argument(
    '--map-file', metavar='FILE', help='stomp map file path')

regionization = stomp_map.add_mutually_exclusive_group()
regionization.add_argument(
    '--map-region-no', metavar='INT', type=TypeNone(int),
    help='number of regions for bootstrapping (default: %d)' %
    DEFAULT_REGION_NO)
regionization.add_argument(
    '--map-region-size', metavar='FLOAT', type=TypeNone(float),
    help='approximate region size for bootstrapping in sq. deg')

analysis_parameters = parser.add_argument_group(
    title='analysis parameters',
    description='controlling the autocorrelation computation')
analysis_parameters.add_argument(
    '--scales-min', metavar='LIST',
    help='comma separated list of inner radii of analysis annulus '
         'on sky in kpc')
analysis_parameters.add_argument(
    '--scales-max', metavar='LIST',
    help='comma separated list of outer radii of analysis annulus '
         'on sky in kpc')
analysis_parameters.add_argument(
    '--across-regions', metavar='Y/N', default='N',
    help='wether to include pairs with galaxies from neighboring spatial '
         'regions')
analysis_parameters.add_argument(
    '--resampling', metavar='INT', type=TypeNone(int),
    help='number of random galaxy realizations created (default: %d)' %
    DEFAULT_RESAMPLING)
analysis_parameters.add_argument(
    '--pair-weighting', metavar='Y/N', type=TypeNone(str),
    help='wether to weight pairs by the inverse separation (default: %s)' %
    DEFALUT_PAIR_WEIGHTING)
analysis_parameters.add_argument(
    '--R-D-ratio', default="local",
    help='ratio of random to data objects (default: local)')


if __name__ == "__main__":

    args = parser.parse_args()

    if args.threads is not None:
        setattr(args, "threads", max(1, min(cpu_count(), args.threads)))

    # load the binning
    try:
        binning = np.loadtxt(args.binning_file)
        assert(np.all(np.diff(binning) > 0.0))
    except Exception:
        raise ValueError("is not a valid binning file: " + args.binning_file)

    # load parameters from the-wizz
    if args.param_file is None:
        for arg in ("cat_file", "cat_ra", "cat_dec", "cat_z",
                    "rand_file", "rand_ra", "rand_dec", "rand_z"
                    "map_file", "scales_min", "scales_max"):
            if getattr(args, arg) is None:
                raise ValueError(
                    "without --param-file, the argument '%s' is required" %
                    ("--" + arg.replace("_", "-")))
        if args.cat_file_ext is None:
            args.cat_file_ext = DEFAULT_CAT_EXT
        if args.rand_file_ext is None:
            args.rand_file_ext = DEFAULT_RAND_EXT
        if args.map_region_no is None and args.map_region_size is None:
            args.map_region_no = DEFAULT_REGION_NO
        if args.resampling is None:
            args.resampling = DEFAULT_RESAMPLING
        if args.pair_weighting is None:
            args.pair_weighting = DEFALUT_PAIR_WEIGHTING
    else:
        wizz_params = load_argparse_summary(args.param_file)
        # populate argument parser namespace with original argument names
        argnames = list(vars(args))
        for arg in argnames:
            if arg in ("wdir", "binning_file", "which",
                       "yaw_param", "rand_weight"):
                continue
            key = arg.replace("cat", args.which)
            try:
                if getattr(args, arg) is None:
                    setattr(args, arg, wizz_params[key])
            except KeyError:
                pass
    if args.threads is None:
        setattr(args, "threads", cpu_count())

    # make working dir if not existing
    setattr(args, "wdir", os.path.abspath(os.path.expanduser(args.wdir)))
    sys.stdout.write("set output folder to %s\n" % args.wdir)
    sys.stdout.flush()
    outdir = CCFolder(args.wdir)
    shutil.copyfile(args.binning_file, outdir.path_binning_file())

    # load the samples
    print("==> loading data objects")
    with pyfits.open(args.cat_file) as fits:
        data = pd.DataFrame({
            "RA": fits[1].data[args.cat_ra].byteswap().newbyteorder(),
            "DEC": fits[1].data[args.cat_dec].byteswap().newbyteorder(),
            "z": fits[1].data[args.cat_z].byteswap().newbyteorder()})
        if args.cat_weight is not None:
            data["weight"] = \
                fits[1].data[args.cat_weight].byteswap().newbyteorder()
    print("==> loading random objects")
    with pyfits.open(args.rand_file) as fits:
        rand = pd.DataFrame({
            "RA": fits[1].data[args.rand_ra].byteswap().newbyteorder(),
            "DEC": fits[1].data[args.rand_dec].byteswap().newbyteorder(),
            "z": fits[1].data[args.rand_z].byteswap().newbyteorder()})
        if args.rand_weight is not None:
            rand["weight"] = \
                fits[1].data[args.rand_weight].byteswap().newbyteorder()

    rlimits = [
        (float(rmin), float(rmax)) for rmin, rmax in
        zip(args.scales_min.split(","), args.scales_max.split(","))]
    scale_names = [outdir.add_scale(rlim) for rlim in rlimits]

    sys.stdout.write("==> run yet another wizz\n")
    sys.stdout.flush()

    for scale_name, (rmin, rmax) in zip(scale_names, rlimits):
        scaledir = outdir[scale_name]
        sys.stdout.write("==> running scale %s\n" % scale_name)
        sys.stdout.flush()
        # create copies of PairMaker for each redshift bin
        est = PairMaker(
            args.map_file, args.map_region_no, threads=args.threads)
        instances = [copy.deepcopy(est) for n in range(len(binning) - 1)]
        # iterate the bins in parallel
        pool = ThreadHelper(len(instances), threads=args.threads)
        pool.add_iterable(data.groupby(pd.cut(data.z, bins=binning)))
        pool.add_iterable(rand.groupby(pd.cut(rand.z, bins=binning)))
        pool.add_constant(args.map_file)
        pool.add_constant((rmin, rmax))
        pool.add_constant(args.R_D_ratio)
        pool.add_constant(args.across_regions == "N")
        pool.add_iterable(instances)
        dataframes = pool.map(run_ac_single_bin)
        # file names of intermediate products
        paircoutdir = Folder(outdir.join("paircounts_" + scale_name))
        meta_file = paircoutdir.join("AC_meta.pkl")
        paircount_file = paircoutdir.join("AC.pqt")
        # write pair count data
        with open(meta_file, "wb") as f:
            pickle.dump(dataframes[0][0], f)
        pd.concat([frames[1] for frames in dataframes]).to_parquet(
            paircount_file)
        # create region pickle
        regionpicklefile = scaledir.path_autocorr_file(
            ".pkl", suffix=args.output_suffix)
        try:
            pdf = PdfMaker(paircount_file, autocorr=True)
            pdf.setBinning(binning)
            pdf.writeRegionPickle(regionpicklefile)
        except ValueError:  # empty pair counts file
            print(
                "WARNING: the pair count file was empty, create dummy "
                "output instead")
            array_shape = (args.ref_bin_no, args.map_region_no)
            pickle_dict = {
                "n_reference": np.zeros(array_shape, dtype=np.int_),
                "redshift": np.zeros(array_shape, dtype=np.float_),
                "unknown": np.zeros(array_shape, dtype=np.float_),
                "rand": np.zeros(array_shape, dtype=np.float_),
                "n_regions": args.map_region_no}
            try:
                zbins = np.diff(
                    getattr(PdfMaker, args.ref_bin_type[0] + "Bins")(
                        args.ref_bin_no, args.z_min, args.z_max))
            except TypeError:
                zbins = np.diff(
                    getattr(PdfMaker, args.ref_bin_type[0] + "Bins")(
                        args.ref_bin_no, args.z_min, args.z_max,
                        default_cosmology.get()))
            pickle_dict["amplitude_factor"] = np.diff(zbins)
            print(
                "writing dummy region pickle to:\n    %s" %
                regionpicklefile)
            with open(regionpicklefile, "wb") as f:
                pickle.dump(pickle_dict, f)
