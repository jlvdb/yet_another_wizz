#!/usr/bin/env python3
import argparse
import copy
import os
import shutil
import sys
from multiprocessing import cpu_count

import numpy as np
import pandas as pd
from astropy.io import fits as pyfits
from yaw_tools.correlation import run_ac_single_bin, load_argparse_summary
from yaw_tools.region_counts import get_region_number
from yaw_tools.folders import CCFolder, Folder
from yaw_tools.utils import (DEFAULT_PAIR_WEIGHTING, DEFAULT_CAT_EXT,
                             DEFAULT_RESAMPLING, DEFAULT_COSMOLOGY, TypeNone)
from yet_another_wizz import PairMaker
from yet_another_wizz import PdfMaker
from yet_another_wizz.utils import dump_json, ThreadHelper, NAMED_COSMOLOGIES


parser = argparse.ArgumentParser(
    description='Compute an estimate for the galaxy bias of a spectroscopic '
                'data sample by measuring the autocorrelation amplitude at '
                'the same comoving projected radii used for the cross-'
                'correlation measurement with the-wizz.'
                '(Rahman et. al. 2016)')

parser.add_argument(
    'wdir', metavar='DATAFOLDER',
    help='folder in which the output is stored')
parser.add_argument(
    '--binning-file', metavar='FILE', required=True,
    help='path to a custom file defining the redshift bin edges')
parser.add_argument(
    '--which', choices=("ref", "test"), required=True,
    help='which type of autocorrelation to compute')
parser.add_argument(
    '--param-file', metavar='FILE',
    help='file with parameters from a yet_another_wizz run to complete '
         'omitted input')
parser.add_argument(
    '--threads', metavar='INT', type=int)
parser.add_argument(
    '--output-suffix', metavar='str',
    help='suffix to append to the auto-correlation and region counts files')

data_cat = parser.add_argument_group(
    title="data sample catalogue file",
    description="defining FITS file and table data columns")
data_cat.add_argument(
    '--cat-file', metavar='FILE', help='data catalogue file path')
data_cat.add_argument(
    '--cat-file-ext', metavar='INT', type=TypeNone(int),
    help='fits extesion containg data (default: %d)' % DEFAULT_CAT_EXT)
data_cat.add_argument(
    '--cat-ra', metavar='FIELD',
    help='right ascension column name')
data_cat.add_argument(
    '--cat-dec', metavar='FIELD', help='declination column name')
data_cat.add_argument(
    '--cat-z', metavar='FIELD', help='redshift column name')
data_cat.add_argument(
    '--cat-weight', metavar='FIELD', help='object weight column name')
data_cat.add_argument(
    '--cat-region', metavar='FIELD', help='spatial region index column name')

rand_cat = parser.add_argument_group(
    title="random sample catalogue file",
    description="defining FITS file and table data columns")
rand_cat.add_argument(
    '--rand-file', metavar='FILE', required=True,
    help='random catalogue file path')
rand_cat.add_argument(
    '--rand-file-ext', metavar='INT', type=int, default=DEFAULT_CAT_EXT,
    help='fits extesion containg data (default: %(default)s)')
rand_cat.add_argument(
    '--rand-ra', metavar='FIELD', required=True,
    help='right ascension column name')
rand_cat.add_argument(
    '--rand-dec', metavar='FIELD', required=True,
    help='declination column name')
rand_cat.add_argument(
    '--rand-z', metavar='FIELD', required=True,
    help='photometric (estimated) redshift column name')
rand_cat.add_argument(
    '--rand-weight', metavar='FIELD', help='object weight column name')
rand_cat.add_argument(
    '--rand-region', metavar='FIELD', help='spatial region index column name')

analysis_parameters = parser.add_argument_group(
    title='analysis parameters',
    description='controlling the autocorrelation computation')
analysis_parameters.add_argument(
    '--cosmology', metavar='STR', choices=NAMED_COSMOLOGIES,
    help='astropy cosmology to use for distance calculations')
analysis_parameters.add_argument(
    '--scales-min', metavar='LIST',
    help='comma separated list of inner radii of analysis annulus '
         'on sky in kpc')
analysis_parameters.add_argument(
    '--scales-max', metavar='LIST',
    help='comma separated list of outer radii of analysis annulus '
         'on sky in kpc')
analysis_parameters.add_argument(
    '--across-regions', metavar='Y/N', default='N',
    help='wether to include pairs with galaxies from neighboring spatial '
         'regions')
analysis_parameters.add_argument(
    '--resampling', metavar='INT', type=TypeNone(int),
    help='number of random galaxy realizations created (default: %d)' %
    DEFAULT_RESAMPLING)
analysis_parameters.add_argument(
    '--pair-weighting', metavar='Y/N', type=TypeNone(str),
    help='wether to weight pairs by the inverse separation (default: %s)' %
    DEFAULT_PAIR_WEIGHTING)
analysis_parameters.add_argument(
    '--R-D-ratio', default="local",
    help='ratio of random to data objects (default: local)')


if __name__ == "__main__":

    args = parser.parse_args()

    if args.threads is not None:
        setattr(args, "threads", max(1, min(cpu_count(), args.threads)))

    # load the binning
    try:
        binning = np.loadtxt(args.binning_file)
        assert(np.all(np.diff(binning) > 0.0))
    except Exception:
        raise ValueError("is not a valid binning file: " + args.binning_file)

    # load parameters from the-wizz
    if args.param_file is None:
        for arg in ("cat_file", "cat_ra", "cat_dec", "cat_z",
                    "scales_min", "scales_max"):
            if getattr(args, arg) is None:
                raise ValueError(
                    "without --param-file, the argument '%s' is required" %
                    ("--" + arg.replace("_", "-")))
        if args.cat_file_ext is None:
            args.cat_file_ext = DEFAULT_CAT_EXT
        if args.rand_file_ext is None:
            args.rand_file_ext = DEFAULT_CAT_EXT
        if args.resampling is None:
            args.resampling = DEFAULT_RESAMPLING
        if args.pair_weighting is None:
            args.pair_weighting = DEFAULT_PAIR_WEIGHTING
        if args.cosmology is None:
            args.cosmology = DEFAULT_COSMOLOGY
    else:
        wizz_params = load_argparse_summary(args.param_file)
        # populate argument parser namespace with original argument names
        argnames = list(vars(args))
        for arg in argnames:
            if arg in ("wdir", "binning_file", "which", "yaw_param"):
                continue
            elif arg.startswith("rand"):
                continue  # depending on --which copying these might be wrong
            key = arg.replace("cat", args.which)
            try:
                if getattr(args, arg) is None:
                    setattr(args, arg, wizz_params[key])
            except KeyError:
                pass
    if args.threads is None:
        setattr(args, "threads", cpu_count())

    # check region column arguments
    regions_set = [
        val is not None for val in (
            args.cat_region, args.rand_region)]
    if any(regions_set) and not all(regions_set):
        raise parser.error(
            "if region indices (--*-region) are used for one data set they "
            "are required for all data sets")

    # warning if one is used with and one without weights
    if args.cat_weight is None and args.rand_weight is not None:
        print("WARNING: running with data but without random weights")
    if args.rand_weight is None and args.cat_weight is not None:
        print("WARNING: running with random but without data weights")

    # make working dir if not existing
    setattr(args, "wdir", os.path.abspath(os.path.expanduser(args.wdir)))
    sys.stdout.write("set output folder to %s\n" % args.wdir)
    sys.stdout.flush()
    outdir = CCFolder(args.wdir)
    shutil.copyfile(args.binning_file, outdir.path_binning_file())

    # load the samples
    print("==> loading data objects")
    with pyfits.open(args.cat_file) as fits:
        fits_data = fits[args.cat_file_ext].data
        data = pd.DataFrame({
            "RA": fits_data[args.cat_ra].byteswap().newbyteorder(),
            "DEC": fits_data[args.cat_dec].byteswap().newbyteorder(),
            "Z": fits_data[args.cat_z].byteswap().newbyteorder()})
        if args.cat_weight is not None:
            data["weights"] = \
                fits_data[args.cat_weight].byteswap().newbyteorder()
        if args.cat_region is not None:
            endian_corrected = \
                fits_data[args.cat_region].byteswap().newbyteorder()
            data["region_idx"] = endian_corrected.astype(np.int16)
    print("==> loading random objects")
    with pyfits.open(args.rand_file) as fits:
        fits_data = fits[args.rand_file_ext].data
        rand = pd.DataFrame({
            "RA": fits_data[args.rand_ra].byteswap().newbyteorder(),
            "DEC": fits_data[args.rand_dec].byteswap().newbyteorder(),
            "Z": fits_data[args.rand_z].byteswap().newbyteorder()})
        if args.rand_weight is not None:
            rand["weights"] = \
                fits_data[args.rand_weight].byteswap().newbyteorder()
        if args.rand_region is not None:
            endian_corrected = \
                fits_data[args.rand_region].byteswap().newbyteorder()
            rand["region_idx"] = endian_corrected.astype(np.int16)
    # get the number of spatial regions from the random data region indices
    n_regions = get_region_number(rand)

    rlimits = [
        (float(rmin), float(rmax)) for rmin, rmax in
        zip(args.scales_min.split(","), args.scales_max.split(","))]
    scale_names = [outdir.add_scale(rlim) for rlim in rlimits]

    sys.stdout.write("==> run yet another wizz\n")
    sys.stdout.flush()

    for scale_name, (rmin, rmax) in zip(scale_names, rlimits):
        scaledir = outdir[scale_name]
        sys.stdout.write("==> running scale %s\n" % scale_name)
        sys.stdout.flush()
        # create copies of PairMaker for each redshift bin
        est = PairMaker(threads=args.threads)
        est.setCosmology(args.cosmology)
        # write the used cosmology
        cosmo_file = outdir.path_cosmology_file()
        if not os.path.exists(cosmo_file):
            est.writeCosmology(cosmo_file)
        instances = [copy.deepcopy(est) for n in range(len(binning) - 1)]

        # iterate the bins in parallel
        pool = ThreadHelper(len(instances), threads=args.threads)
        pool.add_iterable(data.groupby(pd.cut(data.Z, bins=binning)))
        pool.add_iterable(rand.groupby(pd.cut(rand.Z, bins=binning)))
        pool.add_constant((rmin, rmax))
        pool.add_constant(args.R_D_ratio)
        pool.add_constant(args.across_regions == "N")
        pool.add_iterable(instances)
        dataframes = pool.map(run_ac_single_bin)
        # file names of intermediate products
        paircoutdir = Folder(outdir.join("paircounts_" + scale_name))
        meta_file = paircoutdir.join("AC_meta.json")
        paircount_file = paircoutdir.join("AC.pqt")
        # write pair count data
        meta_data = dataframes[0][0]  # same for each bin
        dump_json(meta_data, meta_file)
        pd.concat([frames[1] for frames in dataframes]).to_parquet(
            paircount_file)

        # create region counts
        regioncountsfile = scaledir.path_autocorr_file(
            ".json", suffix=args.output_suffix)
        try:
            pdf = PdfMaker(paircount_file, autocorr=True)
            pdf.setCosmology(args.cosmology)
            pdf.setBinning(binning)
            pdf.writeRegionDict(regioncountsfile)
        except ValueError:  # empty pair counts file
            print(
                "WARNING: the pair count file was empty, create dummy "
                "output instead")
            array_shape = (args.ref_bin_no, n_regions)
            counts_dict = {
                "n_reference": np.zeros(array_shape, dtype=np.int_),
                "sum_redshifts": np.zeros(array_shape, dtype=np.float_),
                "data_data": np.zeros(array_shape, dtype=np.float_),
                "data_random": np.zeros(array_shape, dtype=np.float_),
                "n_regions": n_regions}
            try:
                zbins = np.diff(
                    getattr(PdfMaker, args.ref_bin_type[0] + "Bins")(
                        args.ref_bin_no, args.z_min, args.z_max))
            except TypeError:
                zbins = np.diff(
                    getattr(PdfMaker, args.ref_bin_type[0] + "Bins")(
                        args.ref_bin_no, args.z_min, args.z_max,
                        pdf.getCosmology()))
            counts_dict["amplitude_factor"] = np.diff(zbins)
            print(
                "writing dummy region counts to:\n    %s" %
                regioncountsfile)
            dump_json(counts_dict, regioncountsfile)
