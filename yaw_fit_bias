#!/usr/bin/env python3
import argparse
import os
import pickle

import numpy as np
from matplotlib import pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
from scipy.optimize import curve_fit
from Nz_Fitting import (BiasFitModel, CurveFit, PowerLawBias, RedshiftData,
                        RedshiftDataBinned)
from yaw_tools.folders import (DEFAULT_EXT_BOOT, DEFAULT_EXT_COV,
                               DEFAULT_EXT_DATA, CCFolder, find_cc_scales,
                               init_input_folder, init_output_folder)
from yaw_tools.utils import guess_bin_order


parser = argparse.ArgumentParser(
    description='Estimate the cross-correlation bias by fitting the weighted '
                'sum of redshift bins to the full data sample.')
parser.add_argument(
    'wdir', metavar='DATAFOLDER',
    help='an output folder of yet_another_wizz')
parser.add_argument(
    '--master-key', required=True,
    help='redshift keys (e.g. 0.101z1.201) that identifies the measurement '
         'on the full data sample (only used, if bias is fitted)')
parser.add_argument(
    '--use-cov', action='store_true',
    help='use the covariance matrix instead of the standard errors')
parser.add_argument(
    '-o', '--output',
    help='folder in which the output is stored (optional)')


if __name__ == "__main__":

    args = parser.parse_args()
    indir = init_input_folder(args)
    outdir = init_output_folder(args, indir)
    scales = find_cc_scales(indir)

    # fit the redshift bias
    for scale, scaledir in indir.iter_scales():
        print("==> processing scale: %s" % scale)

        # check which cross-correlation data files exist
        cc_data = scaledir.list_crosscorr_files(DEFAULT_EXT_DATA)
        if len(cc_data) == 0:
            raise ValueError("no cross-correlation data files found")
        elif args.master_key not in cc_data:
            raise KeyError("master key '%s' data not found" % args.master_key)
        # check which cross-correlation bootstrap files exist
        cc_boot = scaledir.list_crosscorr_files(DEFAULT_EXT_BOOT)
        if len(cc_boot) == 0:
            raise ValueError("no cross-correlation bootstraps found")
        if cc_data.keys() != cc_boot.keys():
            raise KeyError(
                "redshift bins of bootstraps do not match data files")
        # check the covariance matrices
        if args.use_cov:
            cc_cov = scaledir.list_crosscorr_files(DEFAULT_EXT_COV)
            if len(cc_cov) == 0:
                raise ValueError(
                    "no cross-correlation covariance matrices found")
            elif args.master_key not in cc_cov:
                raise KeyError(
                    "master key '%s' covariance matrix not found" %
                    args.master_key)
        else:
            cc_cov = None

        # try using the global bin order
        order_file = scaledir.path_bin_order_file()
        if os.path.exists(order_file):
            bin_order = []
            with open(order_file) as f:
                for line in f.readlines():
                    zbin = line.strip("\n")
                    if len(zbin) > 0:
                        bin_order.append(zbin)
        else:
            bin_order = guess_bin_order(cc_data.keys())

        # load the bin weights
        weight_file = scaledir.path_weights_file()
        if not os.path.exists(weight_file):
            raise OSError("bin weights file not found")
        with open(weight_file, "rb") as f:
            weight_dict = pickle.load(f)
        # normalize the weights
        weight_dict.pop(args.master_key)
        norm = sum(weight_dict.values())

        # load the data files
        master_data = None
        bins_data = []
        weights = []
        for zbin in bin_order:
            data = RedshiftData(*np.loadtxt(cc_data[zbin]).T)
            data.setRealisations(np.loadtxt(cc_boot[zbin])) 
            if cc_cov is not None:
                data.setCovariance(np.loadtxt(cc_cov[args.master_key]))
            if zbin == args.master_key:
                master_data = data
            else:
                weights.append(weight_dict[zbin] / norm)
                bins_data.append(data)

        bias = PowerLawBias()
        # construct the fit model
        model = BiasFitModel(bias, bins_data, master_data, weights)

        # fit the bias model
        optimizer = CurveFit(master_data, model)
        fitparams = optimizer.optimize()
        print(fitparams)

        # plot the data and best-fit model
        master_data.plot()
        model.plot(fitparams)
        plt.xlabel(r"$z$", fontsize=13)
        plt.ylabel(r"$p(z)$", fontsize=13)
        plt.grid(alpha=0.25)
        plt.show()
        plt.close()

        fitparams.plotSamples()
        plt.show()
        plt.close()

        # compute bias corrected redshift distributions
        joint_original = RedshiftDataBinned(bins_data, master_data)
        data_corrected = []
        for data in joint_original.data:
            norm_original = np.trapz(data.n, x=data.z)
            # apply bias correction
            bias_eval = bias.modelBest(fitparams, data.z)[1]
            data_reals = np.empty_like(data.reals)
            for i, params in enumerate(fitparams.paramSamples()):
                bias_real = bias.modelBest(params, data.z)[1]
                data_real = data.reals[i] / bias_real
                norm = np.trapz(data_real, x=data.z)
                data_reals[i] = data_real * norm_original / norm
            norm = np.trapz(data.n / bias_eval, x=data.z)
            new_data = RedshiftData(
                data.z, data.n / bias_eval * norm_original / norm,
                np.std(data_reals, axis=0))
            data_corrected.append(new_data)
        joint_corrected = RedshiftDataBinned(
            data_corrected[:-1], data_corrected[-1])
        # make the plot
        fig = joint_original.plot(color="C3")
        joint_corrected.plot(fig=fig, color="C0")
        plt.show()
        plt.close()

        # plot the bias model against the photometric autocorrelation function
        autocorr = scaledir.list_autocorr_files(".dat")
        if "phot" in autocorr:
            ax = plt.gca()
            autocorr_data = np.loadtxt(autocorr["phot"])
            ax.errorbar(
                *autocorr_data.T, color="k", marker=".", ls="none")
            # evluate the bias model and take it's square root
            zmax = master_data.z.max() + 0.1
            z = np.linspace(0.0, zmax, 200)
            sqrt_bias = np.sqrt(bias.modelBest(fitparams, z)[1])
            sqrt_bias_err = np.sqrt(bias.modelError(fitparams, z)[1:])
            # compute the normalisation
            bias_norm = bias.modelBest(fitparams, autocorr_data[:, 0])[0]
            norm = curve_fit(
                lambda z, *args: args[0] * bias_norm,
                z, autocorr_data[:, 1], p0=[1.0])[0][0]
            line = ax.plot(z, sqrt_bias * norm)[0]
            ax.fill_between(
                z, sqrt_bias_err[0] * norm, sqrt_bias_err[1] * norm,
                alpha=0.3, color=line.get_color())
            # finalize plot
            ax.set_xlim(0.0, zmax)
            plt.show()
            plt.close()
