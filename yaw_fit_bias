#!/usr/bin/env python3
import argparse
import os
import pickle

import numpy as np
from matplotlib import pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
from scipy.optimize import curve_fit
from yaw_tools.folders import (DEFAULT_EXT_BOOT, DEFAULT_EXT_COV,
                               DEFAULT_EXT_DATA, CCFolder, find_cc_scales,
                               init_input_folder, init_output_folder)
from yaw_tools.utils import (apply_bias, guess_bin_order, nancov,
                             write_parameters)

from Nz_Fitting import (BiasFitModel, CurveFit, PowerLawBias, RedshiftData,
                        RedshiftDataBinned)


parser = argparse.ArgumentParser(
    description='Estimate the cross-correlation bias by fitting the weighted '
                'sum of redshift bins to the full data sample.')
parser.add_argument(
    'wdir', metavar='DATAFOLDER',
    help='an output folder of yet_another_wizz')
parser.add_argument(
    '--master-key', required=True,
    help='redshift keys (e.g. 0.101z1.201) that identifies the measurement '
         'on the full data sample (only used, if bias is fitted)')
parser.add_argument(
    '--use-cov', action='store_true',
    help='use the covariance matrix instead of the standard errors')
parser.add_argument(
    '-o', '--output',
    help='folder in which the output is stored (optional)')


if __name__ == "__main__":

    args = parser.parse_args()
    indir = init_input_folder(args)
    outdir = init_output_folder(args, indir)
    scales = find_cc_scales(indir)

    # fit the redshift bias
    for scale, scaledir in indir.iter_scales():
        print("==> processing scale: %s" % scale)

        # check which cross-correlation data files exist
        cc_data = scaledir.list_crosscorr_files(DEFAULT_EXT_DATA)
        if len(cc_data) == 0:
            raise ValueError("no cross-correlation data files found")
        elif args.master_key not in cc_data:
            raise KeyError("master key '%s' data not found" % args.master_key)
        # check which cross-correlation bootstrap files exist
        cc_boot = scaledir.list_crosscorr_files(DEFAULT_EXT_BOOT)
        if len(cc_boot) == 0:
            raise ValueError("no cross-correlation bootstraps found")
        if cc_data.keys() != cc_boot.keys():
            raise KeyError(
                "redshift bins of bootstraps do not match data files")
        # check the covariance matrices
        if args.use_cov:
            cc_cov = scaledir.list_crosscorr_files(DEFAULT_EXT_COV)
            if len(cc_cov) == 0:
                raise ValueError(
                    "no cross-correlation covariance matrices found")
            elif args.master_key not in cc_cov:
                raise KeyError(
                    "master key '%s' covariance matrix not found" %
                    args.master_key)
        else:
            cc_cov = None

        # try using the global bin order
        order_file = scaledir.path_bin_order_file()
        if os.path.exists(order_file):
            bin_order = []
            with open(order_file) as f:
                for line in f.readlines():
                    zbin = line.strip("\n")
                    if len(zbin) > 0:
                        bin_order.append(zbin)
        else:
            bin_order = guess_bin_order(cc_data.keys())

        # load the bin weights
        weight_file = scaledir.path_weights_file()
        if not os.path.exists(weight_file):
            raise OSError("bin weights file not found")
        with open(weight_file, "rb") as f:
            weight_dict = pickle.load(f)
        # normalize the weights
        weight_dict.pop(args.master_key)
        norm = sum(weight_dict.values())

        # load the data files
        master_data = None
        bins_data = []
        weights = []
        for zbin in bin_order:
            data = RedshiftData(*np.loadtxt(cc_data[zbin]).T)
            data.setRealisations(np.loadtxt(cc_boot[zbin])) 
            if cc_cov is not None:
                data.setCovariance(np.loadtxt(cc_cov[args.master_key]))
            if zbin == args.master_key:
                master_data = data
                # load the header key
                with open(cc_data[zbin]) as f:
                    for line in f.readlines():
                        if "col 2 = correlation amplitude" in line:
                            header_line = line.strip("\n")
                            break
            else:
                weights.append(weight_dict[zbin] / norm)
                bins_data.append(data)

        bias_model = PowerLawBias()
        # construct the fit model
        model = BiasFitModel(bias_model, bins_data, master_data, weights)

        # fit the bias model
        optimizer = CurveFit(master_data, model)
        fitparams = optimizer.optimize()
        print(fitparams)
        paramdir = outdir[scale].join("bias_parameters")
        print("writing fit parameters to: %s/" % os.path.basename(paramdir))
        write_parameters(fitparams, paramdir, precision=2, notation="decimal")

        # evaluate the bias model for each parameter realisation
        bias_data = bias_model.modelBest(fitparams, master_data.z)[1]
        bias_realisations = np.empty((len(fitparams), len(master_data)))
        for i, params in enumerate(fitparams.paramSamples()):
            bias_realisations[i] = bias_model.modelBest(
                params, master_data.z)[1]
        # collect them as a container
        bias_data = RedshiftData(
            master_data.z, bias_data,
            np.zeros_like(master_data.z))  # compute the correct errors later
        bias_data.setRealisations(bias_realisations)

        # apply the bias correction to the input data
        bins_corrected = [
            apply_bias(data, bias_data) for data in bins_data]
        # finally apply the renormalisation to the bias to get the correct
        # bias model error estimate
        master_corrected = apply_bias(master_data, bias_data, renorm_bias=True)
        joint_corrected = RedshiftDataBinned(bins_corrected, master_corrected)

        # compute the missing covariances
        bias_data.dn = np.std(bias_data.getRealisations(), axis=0)
        bias_data.setCovariance(nancov(bias_data.getRealisations().T))
        joint_corrected.setCovariance(
            nancov(joint_corrected.getRealisations().T))

        # write the bias model files
        outpath = outdir[scale].path_bias_file(".*")
        print("writing bias data to: %s" % os.path.basename(outpath))
        header = "col 1 = mean redshift\n"
        header += "col 2 = bias model\n"
        header += "col 3 = bias error"
        np.savetxt(
            outdir[scale].path_bias_file(DEFAULT_EXT_DATA),
            np.stack([bias_data.z, bias_data.n, bias_data.dn]).T,
            header=header)
        np.savetxt(
            outdir[scale].path_bias_file(DEFAULT_EXT_BOOT),
            bias_data.getRealisations(), header="bias realisations")
        np.savetxt(
            outdir[scale].path_bias_file(DEFAULT_EXT_COV),
            bias_data.getCovariance(), header="bias covariance matrix")

        # write the corrected redshift distributions
        header_key = "%s / bias model" % header_line.strip(")")
        header_key = header_key.split("(")[-1]
        for zbin, data in zip(bin_order, [*bins_corrected, master_corrected]):
            outpath = outdir[scale].path_crosscorr_file(".*", zbin)
            print("writing corrected data to: %s" % os.path.basename(outpath))
            header = "col 1 = mean redshift\n"
            header += "col 2 = correlation amplitude (%s)\n" % header_key
            header += "col 3 = amplitude error"
            np.savetxt(
                outpath.replace(".*", DEFAULT_EXT_DATA),
                np.stack([data.z, data.n, data.dn]).T, header=header)
            np.savetxt(
                outpath.replace(".*", DEFAULT_EXT_BOOT),
                data.getRealisations(),
                header="correlation amplitude (%s) realisations" % header_key)
            np.savetxt(
                outpath.replace(".*", DEFAULT_EXT_COV),
                data.getCovariance(),
                header=(
                    "correlation amplitude (%s) covariance matrix" %
                    header_key))

        # store global covariance matrix
        outpath = outdir[scale].path_global_cov_file("crosscorr")
        print(
            "writing global covariance matrix to: %s" %
            os.path.basename(outpath))
        header = "global correlation amplitude "
        header += "(%s) covariance matrix" % header_key
        np.savetxt(outpath, joint_corrected.getCovariance(), header=header)
        # store the order for later use
        outpath = outdir[scale].path_bin_order_file()
        with open(outpath, "w") as f:
            for zbin in bin_order:
                f.write("%s\n" % zbin)

        # make check plots
        plot_file = outdir[scale].path_bias_file(".pdf")
        print("writing fit check plots to: %s" % os.path.basename(plot_file))
        with PdfPages(plot_file) as pdf:
            # plot the data and best-fit model
            fig = master_data.plot()
            model.plot(fitparams, ax=fig.axes[0])
            fig.set_xlabel(r"$z$", fontsize=13)
            fig.set_ylabel(r"$w(z)$", fontsize=13)
            fig.no_gaps()
            pdf.savefig(fig.fig)
            plt.close(fig.fig)
            # plot the fit parameter samples
            fig = fitparams.plotSamples()
            pdf.savefig(fig)
            plt.close(fig)
