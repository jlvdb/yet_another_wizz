#!/usr/bin/env python3
import argparse
import os
import pickle
import shutil
import sys

import numpy as np


parser = argparse.ArgumentParser(
    description='Merges region pickle files produced by yet_another-wizz')
parser.add_argument(
    '-i', '--input', nargs='*', required=True,
    help='input folders which contain output of yet_another')
parser.add_argument(
    '--scale', help='only merge this specific scale (ignores missing input)')
parser.add_argument(
    '-o', '--output', required=True,
    help='path to folder where merged output is stored')


def merge_pickles(pickle_list):
    pickles = []
    for path in pickle_list:
        with open(path, "rb") as f:
            pickles.append(pickle.load(f))
    # create the master pickle
    master_pickle = {"n_regions": 0}
    for key in pickles[0]:
        try:
            master_pickle[key] = np.concatenate(
                [data[key] for data in pickles], axis=1)
        except ValueError:
            if key == "n_regions":  # sum the region counters
                master_pickle[key] = sum(data[key] for data in pickles)
            else:  # just copy first from first element
                master_pickle[key] = pickles[0][key]
    return master_pickle


if __name__ == "__main__":
    args = parser.parse_args()

    # check if all the input exists
    folders = []
    for folder in args.input:
        abspath = os.path.abspath(os.path.expanduser(folder))
        if not os.path.exists(abspath):
            raise OSError("input folder not found: %s" % abspath)
        folders.append(abspath)
    print("==> processing %d input folders" % len(folders))

    print("checking binning")
    for folder in folders:
        try:
            binning_file = os.path.join(folder, "binning.yaw")
            next_binning = np.loadtxt(binning_file)
            assert(np.all(binning == next_binning))
        except NameError:
            binning = next_binning
        except AssertionError:
            raise ValueError("binning does not match: %s" % binning_file)
        except OSError:
            raise ValueError(
                "folder does not contain yet_another_wizz output: %s" % folder)

    # check which scales coexist
    print("finding correlation scales")
    for folder in folders:
        next_scales = set(
            f for f in os.listdir(folder) if f.startswith("kpc"))
        try:
            common_scales &= next_scales
            all_scales |= next_scales
        except NameError:
            common_scales = set(s for s in next_scales)
            all_scales = set(s for s in next_scales)
    print("found scales: %s" % str(all_scales))
    if args.scale is None:
        print("selecting:    %s" % str(common_scales))
        scales = list(common_scales)
    else:  # use only requested scale
        print("selecting:    %s" % str(args.scale))
        if args.scale not in all_scales:
            raise ValueError("scale '%s' not found" % args.scale)
        if args.scale not in common_scales:
            print(
                "WARNING: scale '%s' is not common to all input" % args.scale)
        scales = [args.scales]

    # list folders that have the requested scale
    scale_folders = {}
    for scale in scales:
        scale_folders[scale] = []
        for folder in folders:
            scale_folder = os.path.join(folder, scale)
            if os.path.exists(scale_folder):
                scale_folders[scale].append(scale_folder)

    # check which pickle types are to be merged
    merge_autocorr = {}
    merge_crosscorr = {}
    for scale, in_folders in scale_folders.items():
        autocorr = np.full(len(in_folders), False, dtype="bool")
        crosscorr = np.full(len(in_folders), False, dtype="bool")
        # check which pickle types exist
        for i, folder in enumerate(in_folders):
            for path in os.listdir(folder):
                if path.startswith("autocorr") and path.endswith(".pickle"):
                    autocorr[i] = True
                if path.startswith("crosscorr") and path.endswith(".pickle"):
                    crosscorr[i] = True
        # check that all folders have auto-correlations
        if np.all(autocorr):
            merge_autocorr[scale] = True
        elif np.any(autocorr):
            raise ValueError(
                "missing autocorrelation pickles in %d / %d input folders" %
                (np.count_nonzero(~autocorr), len(autocorr)))
        else:
            merge_autocorr[scale] = False
        # check that all folders have cross-correlations
        if np.all(crosscorr):
            merge_crosscorr[scale] = True
        elif np.any(crosscorr):
            raise ValueError(
                "missing autocorrelation pickles in %d / %d input folders" %
                (np.count_nonzero(~crosscorr), len(crosscorr)))
        else:
            merge_crosscorr[scale] = False

    if all(not cc for cc in merge_crosscorr.values()) and \
            all(not ac for ac in merge_autocorr.values()):
        sys.exit("WARNING: there is nothing to do")

    outdir = os.path.abspath(os.path.expanduser(args.output))
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    binning_copy = os.path.join(outdir, "binning.yaw")
    if not os.path.exists(binning_copy):
        shutil.copy(binning_file, binning_copy)
    else:
        old_binning = np.loadtxt(binning_copy)
        new_binning = np.loadtxt(binning_file)
        if not np.all(old_binning == new_binning):
            raise ValueError(
                "data in output folder exists but has different redshift " +
                "binning: " + outdir)
    # merge the pickles
    for scale in scales:
        print("==> processing scale: %s" % scale)
        scaledir = os.path.join(outdir, scale)
        if not os.path.exists(scaledir):
            os.makedirs(scaledir)

        if merge_autocorr[scale]:
            print("merging auto-correlation pickles")
            pickle_paths = {}
            for folder in scale_folders[scale]:
                pickles = [
                    os.path.join(folder, path) for path in os.listdir(folder)
                    if path.startswith("autocorr") and
                    path.endswith(".pickle")]
                for path in pickles:
                    try:
                        base = os.path.basename(path)
                        name = os.path.splitext(base)[0]
                        typename = name.split("_")[1]
                    except IndexError:
                        typename = "[none]"
                    if typename not in pickle_paths:
                        pickle_paths[typename] = []
                    pickle_paths[typename].append(path)
            print(
                "found auto-correlation type: %s" %
                str(sorted(pickle_paths.keys())))
            for typename, paths in pickle_paths.items():
                try:
                    assert(len(scale_folders[scale]) == len(paths))
                except AssertionError:
                    raise ValueError(
                        "expected %d input files for type '%s' but got %d" %
                        (len(scale_folders[scale]), typename, len(paths)))
            for typename, paths in pickle_paths.items():
                master_pickle = merge_pickles(paths)
                if typename == "[none]":
                    outpath = os.path.join(scaledir, "autocorr.pickle")
                else:
                    outpath = os.path.join(
                        scaledir, "autocorr_%s.pickle" % typename)
                print("writing merged pickle pickle: %s" % outpath)
                with open(outpath, "wb") as f:
                    pickle.dump(master_pickle, f)

        if merge_crosscorr[scale]:
            print("merging cross-correlation pickles")
            pickle_paths = {}
            for folder in scale_folders[scale]:
                pickles = [
                    os.path.join(folder, path) for path in os.listdir(folder)
                    if path.startswith("crosscorr") and
                    path.endswith(".pickle")]
                for path in pickles:
                    try:
                        base = os.path.basename(path)
                        name = os.path.splitext(base)[0]
                        binname = name.split("_")[1]
                    except IndexError:
                        binname = "ALL"
                    if binname not in pickle_paths:
                        pickle_paths[binname] = []
                    pickle_paths[binname].append(path)
            print(
                "found tomographic bins: %s" %
                str(sorted(pickle_paths.keys())))
            for binname, paths in pickle_paths.items():
                try:
                    assert(len(scale_folders[scale]) == len(paths))
                except AssertionError:
                    raise ValueError(
                        "expected %d input files for bin '%s' but got %d" %
                        (len(scale_folders[scale]), binname, len(paths)))
            for binname, paths in pickle_paths.items():
                master_pickle = merge_pickles(paths)
                if binname == "ALL":
                    outpath = os.path.join(scaledir, "crosscorr.pickle")
                else:
                    outpath = os.path.join(
                        scaledir, "crosscorr_%s.pickle" % binname)
                print("writing merged pickle pickle: %s" % outpath)
                with open(outpath, "wb") as f:
                    pickle.dump(master_pickle, f)

    # sum the bin weights together
    weights_exist = [os.path.exists(
        os.path.join(folder, "bin_weights.pickle")) for folder in folders]
    if not any(weights_exist):
        print("found no tomographic bin weights to sum")
    elif not all(weights_exist):
        raise ValueError("not all input folders have bin weight files")
    else:
        print("compute the combined tomographic bin weights")
        weight_pickles = []
        for folder in folders:
            with open(os.path.join(folder, "bin_weights.pickle"), "rb") as f:
                data = pickle.load(f)
                weight_pickles.append(data)
                try:
                    assert(bin_keys == set(data.keys()))
                except NameError:
                    bin_keys = set(data.keys())
                except AssertionError:
                    raise ValueError(
                        "cannot combine weights of differing tomographic bins")
        master_weight = {key: 0.0 for key in bin_keys}
        for weights in weight_pickles:
            for key in bin_keys:
                master_weight[key] += weights[key]
        print(
            "weight sum:   %s" % {
                k: "%.2e" % v for k, v in master_weight.items()})
        with open(os.path.join(outdir, "bin_weights.pickle"), "wb") as f:
            pickle.dump(master_weight, f)
